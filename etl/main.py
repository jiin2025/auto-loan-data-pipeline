import pandas as pd import boto3 import os from pathlib import Path from dotenv import load_dotenv from airflow.models import Variable from sqlalchemy import create_engine from azure_load import AzureDataLakeHandler # 1. 환경 변수 로드 (로컬 테스트용 .env 포함) load_dotenv(dotenv_path=Path(__file__).parent.parent / '.env') # 2. Configuration (Airflow Variables) TENANT_ID = Variable.get("AZURE_TENANT_ID", default_var=None) CLIENT_ID = Variable.get("AZURE_CLIENT_ID", default_var=None) CLIENT_SECRET = Variable.get("AZURE_CLIENT_SECRET", default_var=None) ACCOUNT_NAME = Variable.get("AZURE_ACCOUNT_NAME", default_var=None) CONTAINER_NAME = Variable.get("AZURE_CONTAINER_NAME", default_var=None) # 3. AWS Configuration (From .env) AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID") AWS_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY") S3_BUCKET = os.getenv("S3_BUCKET_NAME") # Path settings BASE_DIR = Path("/opt/airflow/data") ETL_DIR = Path("/opt/airflow/etl/data") def run_etl(): """Integrated Pipeline: Local -> AWS S3 -> Azure -> PostgreSQL""" # --- [초기 설정 확인] --- if not TENANT_ID or not AWS_ACCESS_KEY: print("Error: Missing Azure or AWS credentials.") return # S3 Client & Azure Handler s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY) azure = AzureDataLakeHandler(TENANT_ID, CLIENT_ID, CLIENT_SECRET, ACCOUNT_NAME, CONTAINER_NAME) entities = ["customers", "loans", "payments"] for entity in entities: print(f"\n--- Processing {entity} ---") local_raw_path = BASE_DIR / f"{entity}.csv" # --- [STEP 1: AWS S3 Backup] --- # 원본을 AWS S3에 먼저 백업합니다. print(f"1. Backing up to AWS S3...") s3_client.upload_file(str(local_raw_path), S3_BUCKET, f"backup/{entity}.csv") # --- [STEP 2: Azure Raw Zone] --- # Azure 메달리온 아키텍처의 시작 (Raw) print(f"2. Uploading to Azure Raw Zone...") azure.upload_file(local_raw_path, f"raw/{entity}/{entity}.csv") # --- [STEP 3: Cleaning & Staging] --- # 데이터 정제 후 Azure Staging에 업로드 df = pd.read_csv(local_raw_path) df_cleaned = df.fillna(0) staging_path = ETL_DIR / "staging" / entity / f"{entity}.csv" staging_path.parent.mkdir(parents=True, exist_ok=True) df_cleaned.to_csv(staging_path, index=False) azure.upload_file(staging_path, f"staging/{entity}/{entity}.csv") print(f"3. Staging complete for {entity}.") # --- [STEP 4: Aggregation (Curated) & DB Load] --- # (여기서부터는 기존에 성공하신 요약 및 Postgres 적재 로직을 그대로 붙여넣으면 됩니다!) print("\n>> Starting Final Aggregation and PostgreSQL Load...") # ... (생략된 기존 요약 로직) ... print("ETL Pipeline Finished Successfully!") if __name__ == "__main__": run_etl()
